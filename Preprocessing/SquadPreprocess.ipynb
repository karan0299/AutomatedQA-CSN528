{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import embedding_ops"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/kps/Desktop/sem7/NLPProject/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kps/Desktop/sem7/NLPProject/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kps/Desktop/sem7/NLPProject/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kps/Desktop/sem7/NLPProject/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kps/Desktop/sem7/NLPProject/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kps/Desktop/sem7/NLPProject/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/kps/Desktop/sem7/NLPProject/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kps/Desktop/sem7/NLPProject/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kps/Desktop/sem7/NLPProject/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kps/Desktop/sem7/NLPProject/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kps/Desktop/sem7/NLPProject/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kps/Desktop/sem7/NLPProject/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def data_from_json(filename):\n",
    "    \"\"\"Loads JSON data from filename and returns\"\"\"\n",
    "    with open(filename) as data_file:\n",
    "        data = json.load(data_file)\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\n",
    "def tokenize(sequence):\n",
    "    tokens = [token.replace(\"``\", '\"').replace(\"''\", '\"').lower() for token in nltk.word_tokenize(sequence)]\n",
    "    return tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def get_char_word_loc_mapping(context, context_tokens):\n",
    "    \"\"\"\n",
    "    Return a mapping that maps from character locations to the corresponding token locations.\n",
    "    If we're unable to complete the mapping e.g. because of special characters, we return None.\n",
    "\n",
    "    Inputs:\n",
    "      context: string (unicode)\n",
    "      context_tokens: list of strings (unicode)\n",
    "\n",
    "    Returns:\n",
    "      mapping: dictionary from ints (character locations) to (token, token_idx) pairs\n",
    "        Only ints corresponding to non-space character locations are in the keys\n",
    "        e.g. if context = \"hello world\" and context_tokens = [\"hello\", \"world\"] then\n",
    "        0,1,2,3,4 are mapped to (\"hello\", 0) and 6,7,8,9,10 are mapped to (\"world\", 1)\n",
    "    \"\"\"\n",
    "    acc = '' # accumulator\n",
    "    current_token_idx = 0 # current word loc\n",
    "    mapping = dict()\n",
    "\n",
    "    for char_idx, char in enumerate(context): # step through original characters\n",
    "        if char != u' ' and char != u'\\n': # if it's not a space:\n",
    "            acc += char # add to accumulator\n",
    "            context_token = str(context_tokens[current_token_idx]) # current word token\n",
    "            if acc == context_token: # if the accumulator now matches the current word token\n",
    "                syn_start = char_idx - len(acc) + 1 # char loc of the start of this word\n",
    "                for char_loc in range(syn_start, char_idx+1):\n",
    "                    mapping[char_loc] = (acc, current_token_idx) # add to mapping\n",
    "                acc = '' # reset accumulator\n",
    "                current_token_idx += 1\n",
    "\n",
    "    if current_token_idx != len(context_tokens):\n",
    "        return None\n",
    "    else:\n",
    "        return mapping"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def write_to_file(out_file, line):\n",
    "    out_file.write(str(line.encode('utf8')) + '\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def preprocess_and_write(dataset, tier):\n",
    "    \"\"\"Reads the dataset, extracts context, question, answer, tokenizes them,\n",
    "    and calculates answer span in terms of token indices.\n",
    "    Note: due to tokenization issues, and the fact that the original answer\n",
    "    spans are given in terms of characters, some examples are discarded because\n",
    "    we cannot get a clean span in terms of tokens.\n",
    "\n",
    "    This function produces the {train/dev}.{context/question/answer/span} files.\n",
    "\n",
    "    Inputs:\n",
    "      dataset: read from JSON\n",
    "      tier: string (\"train\" or \"dev\")\n",
    "      out_dir: directory to write the preprocessed files\n",
    "    Returns:\n",
    "      the number of (context, question, answer) triples written to file by the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    num_exs = 0 # number of examples written to file\n",
    "    num_mappingprob, num_tokenprob, num_spanalignprob = 0, 0, 0\n",
    "    examples = []\n",
    "\n",
    "    for articles_id in tqdm(range(len(dataset['data'])), desc=\"Preprocessing {}\".format(tier)):\n",
    "\n",
    "        article_paragraphs = dataset['data'][articles_id]['paragraphs']\n",
    "        for pid in range(len(article_paragraphs)):\n",
    "\n",
    "            context = str(article_paragraphs[pid]['context']) # string\n",
    "\n",
    "            # The following replacements are suggested in the paper\n",
    "            # BidAF (Seo et al., 2016)\n",
    "            context = context.replace(\"''\", '\" ')\n",
    "            context = context.replace(\"``\", '\" ')\n",
    "\n",
    "            context_tokens = tokenize(context) # list of strings (lowercase)\n",
    "            \n",
    "            context = context.lower()\n",
    "\n",
    "            qas = article_paragraphs[pid]['qas'] # list of questions\n",
    "\n",
    "            charloc2wordloc = get_char_word_loc_mapping(context, context_tokens) # charloc2wordloc maps the character location (int) of a context token to a pair giving (word (string), word loc (int)) of that token\n",
    "            if charloc2wordloc is None: # there was a problem\n",
    "                num_mappingprob += len(qas)\n",
    "                continue # skip this context example\n",
    "\n",
    "            # for each question, process the question and answer and write to file\n",
    "            for qn in qas:\n",
    "\n",
    "                # read the question text and tokenize\n",
    "                question = str(qn['question']) # string\n",
    "                question_tokens = tokenize(question) # list of strings\n",
    "\n",
    "                # of the three answers, just take the first\n",
    "                ans_text = str(qn['answers'][0]['text']).lower() # get the answer text\n",
    "                ans_start_charloc = qn['answers'][0]['answer_start'] # answer start loc (character count)\n",
    "                ans_end_charloc = ans_start_charloc + len(ans_text) # answer end loc (character count) (exclusive)\n",
    "\n",
    "                # Check that the provided character spans match the provided answer text\n",
    "                if context[ans_start_charloc:ans_end_charloc] != ans_text:\n",
    "                  # Sometimes this is misaligned, mostly because \"narrow builds\" of Python 2 interpret certain Unicode characters to have length 2 https://stackoverflow.com/questions/29109944/python-returns-length-of-2-for-single-unicode-character-string\n",
    "                  # We should upgrade to Python 3 next year!\n",
    "                  num_spanalignprob += 1\n",
    "                  continue\n",
    "\n",
    "                # get word locs for answer start and end (inclusive)\n",
    "                ans_start_wordloc = charloc2wordloc[ans_start_charloc][1] # answer start word loc\n",
    "                ans_end_wordloc = charloc2wordloc[ans_end_charloc-1][1] # answer end word loc\n",
    "                assert ans_start_wordloc <= ans_end_wordloc\n",
    "\n",
    "                # Check retrieved answer tokens match the provided answer text.\n",
    "                # Sometimes they won't match, e.g. if the context contains the phrase \"fifth-generation\"\n",
    "                # and the answer character span is around \"generation\",\n",
    "                # but the tokenizer regards \"fifth-generation\" as a single token.\n",
    "                # Then ans_tokens has \"fifth-generation\" but the ans_text is \"generation\", which doesn't match.\n",
    "                ans_tokens = context_tokens[ans_start_wordloc:ans_end_wordloc+1]\n",
    "                if \"\".join(ans_tokens) != \"\".join(ans_text.split()):\n",
    "                    num_tokenprob += 1\n",
    "                    continue # skip this question/answer pair\n",
    "\n",
    "                examples.append((' '.join(context_tokens), ' '.join(question_tokens), ' '.join(ans_tokens), ' '.join([str(ans_start_wordloc), str(ans_end_wordloc)])))\n",
    "\n",
    "                num_exs += 1\n",
    "\n",
    "    print(\"Number of (context, question, answer) triples discarded due to char -> token mapping problems: \", num_mappingprob)\n",
    "    print(\"Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization: \", num_tokenprob)\n",
    "    print(\"Number of (context, question, answer) triples discarded due character span alignment problems: \", num_spanalignprob)\n",
    "    print(\"Processed %i examples of total %i\\n\" % (num_exs, num_exs + num_mappingprob + num_tokenprob + num_spanalignprob))\n",
    "\n",
    "    # shuffle examples\n",
    "    indices = list(range(len(examples)))\n",
    "    np.random.shuffle(indices)\n",
    "    data_path = \"../Data/\" + tier + \"/\"\n",
    "    with open(data_path + 'context', 'w') as context_file,  \\\n",
    "         open(data_path +'question', 'w') as question_file,\\\n",
    "         open(data_path + 'answer', 'w') as ans_text_file, \\\n",
    "         open(data_path +'span', 'w') as span_file:\n",
    "\n",
    "        for i in indices:\n",
    "            (context, question, answer, answer_span) = examples[i]\n",
    "\n",
    "            # write tokenized data to file\n",
    "            write_to_file(context_file, context)\n",
    "            write_to_file(question_file, question)\n",
    "            write_to_file(ans_text_file, answer)\n",
    "            write_to_file(span_file, answer_span)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "# preprocess train set and write to file\n",
    "train_file_path = \"../Data/Train/train-v1.1.json\"\n",
    "train_data = data_from_json(train_file_path)\n",
    "preprocess_and_write(train_data,'Train')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Preprocessing Train: 100%|██████████| 442/442 [00:58<00:00,  7.61it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of (context, question, answer) triples discarded due to char -> token mapping problems:  97\n",
      "Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization:  1173\n",
      "Number of (context, question, answer) triples discarded due character span alignment problems:  23\n",
      "Processed 86306 examples of total 87599\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "dev_file_path = \"../Data/Dev/dev-v1.1.json\"\n",
    "dev_data = data_from_json(dev_file_path)\n",
    "preprocess_and_write(dev_data,'Dev')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Preprocessing Dev: 100%|██████████| 48/48 [00:04<00:00, 11.80it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of (context, question, answer) triples discarded due to char -> token mapping problems:  0\n",
      "Number of (context, question, answer) triples discarded because character-based answer span is unaligned with tokenization:  176\n",
      "Number of (context, question, answer) triples discarded due character span alignment problems:  0\n",
      "Processed 10394 examples of total 10570\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('env': venv)"
  },
  "interpreter": {
   "hash": "1509c729783193412647cd42bd69a2b55286a15b2066396dbcfcccc0a681516f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}